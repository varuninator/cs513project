{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Rohan Ramavajjala, Varun Bharadwaj, and Jason Sim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Dataset\n",
    "\n",
    "With around 45,000 menus dating from the 1840s to today, The New York Public Libraryâ€™s restaurant menu collection is one of the largest globally, attracting historians, chefs, novelists, and food enthusiasts. However, these menus are challenging to search for their most valuable details: specific information about dishes, prices, meal organization, and the historical and cultural stories they reveal. To address this, we will be cleaning this dataset to make it more useful. As a first step, below we have outline the tables of the data available that has been provided by NYPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Menu Table\n",
    "The Menu table stores all the important information about each menu, like who sponsored it, the event it was for, where it was held, and a description of what it looks like, along with other details. Each menu can have several pages, and these pages are stored in the MenuPage table. \n",
    "\n",
    "| Field               | Description                                      |\n",
    "|---------------------|--------------------------------------------------|\n",
    "| **Id**              | Primary key, Menu Identifier                     |\n",
    "| **Name**            | Name of menu (ISNULL)                            |\n",
    "| **Sponsor**         | Sponsor of event where menu is used              |\n",
    "| **Event**           | Event that is associated with menu               |\n",
    "| **Venue**           | The venue that the menu was used                 |\n",
    "| **Place**           | Where the menu was used                          |\n",
    "| **Physical_description** | Description of the menu physically          |\n",
    "| **Occasion**        | Occasion which the menu was created (ISNULL)     |\n",
    "| **Notes**           | Notes about the menu (ISNULL)                    |\n",
    "| **Call_number**     | Call number to call back to                      |\n",
    "| **Keywords**        | Keywords associated with menu (ISNULL)           |\n",
    "| **Language**        | Language of menu (ISNULL)                        |\n",
    "| **Date**            | Date where menu was used                         |\n",
    "| **Location**        | Location of menu                                 |\n",
    "| **Location_type**   | Type of location                                 |\n",
    "| **Currency**        | What currency was used in the menu               |\n",
    "| **Currency_symbol** | Symbol of the currency (ISNULL)                  |\n",
    "| **Status**          | Menu status                                      |\n",
    "| **Page_count**      | How many pages are in the menu                   |\n",
    "| **Dish_count**      | How many dishes are in the menu                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MenuPage Table\n",
    "The MenuPage table contains details about each page. Some of the details include including the a unique identifier for the page image, page number, and the size of the page image\n",
    "\n",
    "| Field          | Description                             |\n",
    "|----------------|-----------------------------------------|\n",
    "| **Id**         | Primary Key, Menu page Id               |\n",
    "| **Menu_id**    | The id of the menu this page belongs to, Foreign Key |\n",
    "| **Page_number**| The page number of the menu             |\n",
    "| **Image_id**   | The id for the image of the page        |\n",
    "| **Full_height**| The full height of the image            |\n",
    "| **Full_width** | The width of the image                  |\n",
    "| **Uuid**       | The unique identifier of the page       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MenuItem Table\n",
    "The MenuItem table tracks the occurrences of a single dish appearing somewhere on a menu page. Some details include price, time of creation and last update as well as the x-y coordinates of the dish on the page.\n",
    "\n",
    "| Field           | Description                                                                    |\n",
    "|-----------------|--------------------------------------------------------------------------------|\n",
    "| **Id**          | Primary Key, Menu Item ID                                                      |\n",
    "| **Menu_page_id**| Foreign Key, Menu Page that the item is associated with                        |\n",
    "| **Price**       | Price of the menu item                                                         |\n",
    "| **High_price**  | The highest price if the item has more than one price on a single menu         |\n",
    "| **Dish_id**     | Id of the dish                                                                 |\n",
    "| **Created_at**  | Date/time of creation                                                          |\n",
    "| **Updated_at**  | Date/time of last update                                                       |\n",
    "| **Xpos**        | X coordinate on the page for the upper left point where the menu item is on the page |\n",
    "| **Ypos**        | Y coordinate on the page for the upper left point where the menu item is on the page |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dish Table\n",
    "The dish table contains all information about a dish (the actual food) on a menu. Some details include the name of the dish, how much it cost, and how many times it has occurred across all menus and the number of menus it has appeared on.\n",
    "\n",
    "| Field            | Description                               |\n",
    "|------------------|-------------------------------------------|\n",
    "| **Id**           | Primary Key, Dish ID                      |\n",
    "| **Name**         | Name of dish                              |\n",
    "| **Description**  | Description of dish                       |\n",
    "| **Menus_appeared**| Number of menus the dish appeared on     |\n",
    "| **Times_appeared**| Number of times the dish has appeared across all menus |\n",
    "| **First_appeared**| The first time the dish appeared         |\n",
    "| **Last_appeared**| The last time the dish appeared           |\n",
    "| **Lowest_price** | The lowest price observed for the dish    |\n",
    "| **Highest_price**| The highest price observed for the dish   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Use Cases\n",
    "\n",
    "We provide use cases describing hypothetical data analysis.\n",
    "\n",
    "#### Target (Main) Use Case U1: \n",
    "\n",
    "Data cleaning is necessary and sufficient for use cases involving analysis where accurate aggregation or filtering across relationships is required. An example would be finding menus that included the 5 most popular dishes for a given time period, with popularity defined by the number of menus the dish has appeared on. The Menu dataset has several columns with missing values (e.g., 'name', 'occasion', 'keywords'), and there are inconsistencies in the date format and categorical data (e.g., venue names). Data cleaning steps include: \n",
    "\n",
    "#### \"Zero Data Cleaning\" Use Case U0: \n",
    "\n",
    "Data cleaning is not necessary in use cases that allow for reading the datasets exactly as they are. For example, if people wanted to see a catalog of entries based on a search for personal observation. One can conduct basic descriptive statistics or other analysis directly on the provided data if they wanted to, for example counting menus by sponsor or year, but then the end result would be suspect. \n",
    "\n",
    "#### \"Never Enough\" Use Case U2: \n",
    "\n",
    "Data cleaning is not sufficient in use cases that involve analysis on fields that are missing too many values, or analysis that requires columns that are missing entirely. An example would be in the dish dataset, no amount of cleaning could fix the fact that the description field is entirely empty, and thus no analysis can be done on it at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Problems\n",
    "\n",
    "The Menu dataset exhibits several data quality issues: \n",
    "\n",
    "1. Missing values in critical columns such as 'name' (14,348 missing out of 17,545), 'occasion' (13,754 missing), and completely empty columns like 'keywords', 'language', and 'location_type'. For instance, row 0 has missing values in 'name', 'event', 'occasion', 'keywords', 'language', and 'currency'. The MenuPage dataset also has missing values in 'page_number' (1,202 missing out of 66,937), 'full_height', and 'full_width' (329 missing each), with row 1200 missing 'page_number'. In MenuItems, price (445,916 missing out of 1,332,726) and dish_id (241 missing out of 1,332,726) has missing values. In the dish dataset, all elementsfor description are missing.\n",
    " 2. The 'date' column in the Menu Tablehas inconsistent formats that need standardization to datetime, e.g., '1900-04-15'. \n",
    " 3. Redundant columns like 'keywords' and 'language' in the Menu Table add no value due to being entirely empty. \n",
    " 4. The dataset also likely contains duplicate entries, which could distort analysis results, exemplified by potential duplicates in 'id', 'name', and 'sponsor' in the Menu Table. \n",
    " 5. Inconsistent categorical data in the Menu Table, such as varying cases in 'venue' (e.g., 'COMMERCIAL' vs 'COMMERCIAL;'), further complicate analysis.  \n",
    "\n",
    "Data cleaning is crucial for supporting the main use case U1 to ensure the dataset's completeness and reliability. Handling missing values, particularly in 'name' and 'occasion', ensures that the dataset is robust for analysis. For example, dropping rows with missing 'name' values ensures every menu entry is identifiable. Ensuring different variants of the same name are merged would increase accuracy in any aggregations across that field Standardizing data formats, like converting 'date' to datetime, facilitates accurate chronological analysis. Removing redundant columns, such as 'keywords' and 'language', simplifies the dataset and focuses on valuable data. Identifying and removing duplicates prevents distorted analysis results, ensuring each entry is unique. Ensuring consistency in categorical data, such as standardizing venue names to uppercase, ensures accurate grouping and analysis. These steps make the dataset accurate and reliable, supporting effective analysis for the main use case U1. Ensuring integrity constraints allows for more accurate querying and prevents confusion when looking at menu and dish data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Plan Steps\n",
    "\n",
    "1.\tWeeks 8 through 11 - Reviewing The Dataset and Use Case: Our dataset includes 4 tables: Menu, Menupage, MenuItem, and dish. The Menu table includes information about physical menus, including historical information, uses, and formats. MenuPages capture the pages of menus, and are modeled after stored digital images of the menus. The MenuItem table tracks the occurrences of a single dish appearing somewhere on a menu page, and the Dish table includes information about all the dishes from all the menus. Our use case is aggregation and filtering across relationships. An example is finding menus that included the 5 most popular dishes for a given time period, with popularity defined by the number of menus the dish has appeared on. All team members will contribute to the review of the dataset and use case.\n",
    "\n",
    "2.\tWeek 8 - Profiling the Dataset: We will use python and OpenRefine to identify cardinalities, data types, value distributions, correlations, constraints, duplicates and syntactic errors. We will use Datalog to help identify violations of integrity constraints, and logical semantic and structural errors. Once we have identified all types of errors, we will create a checklist to go through, and create a list of examples that we can track the improvement in. Jason, Rohan will profile the dataset via OpenRefine. ____ will check for integrity constraints. \n",
    "\n",
    "3.\tWeek 9 through 11 - Performing the data cleaning process: We will be primarily using OpenRefine and Python to implement the data cleaning as it relates to syntactic errors. OpenRefineâ€™s GUI and existing functionality lends itself to solving a lot of syntactic errors, while using the Pandas library in Pytho will provide any addintional flexibility for manipulating data that may be more difficult to do in OpenRefine. As for errors relating to integrity restraints and semantics, Datalog will be especially useful for that. Varun will handle syntactic errors and duplicates via OpenRefine. ____ will enforce and clean data as it relates to integrity constraints.\n",
    "\n",
    "4.\tWeeks 9 through 11 - Checking: We will maintain a list of examples and systematically verify that they are indeed becoming cleaner after each transformation.\n",
    "\n",
    "5.\tWeeks 9 through 11 - Documentation of Change: we will use python to create scripts to quantitatively analyze the improvement in data cleanliness, while also creating visual representations to track improvements along the way.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
